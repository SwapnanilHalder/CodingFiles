{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "615f81ba",
   "metadata": {},
   "source": [
    "Name: Swapnanil Halder, Roll : 18MA20046"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b0786c",
   "metadata": {},
   "source": [
    "# Implementing backpropagation from scratch with mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c53d6529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "class NeuralNet:\n",
    "    \n",
    "    def __init__(self, layers, batch_size, lr=0.1):\n",
    "        self.W = []\n",
    "        self.layers = layers\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        \n",
    "        for i in np.arange(0, len(layers) - 2):\n",
    "            w = np.random.randn(layers[i] + 1, layers[i + 1] + 1)\n",
    "            self.W.append(w / np.sqrt(layers[i]))\n",
    "            \n",
    "        w = np.random.randn(layers[-2] + 1, layers[-1])\n",
    "        self.W.append(w)\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1.0 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def train(self, x, y):\n",
    "        A = [np.atleast_2d(x)]\n",
    "        \n",
    "        #forward pass\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            net = A[layer].dot(self.W[layer])\n",
    "            out = self.sigmoid(net)\n",
    "            A.append(out)\n",
    "            \n",
    "        #calculating gradients in backward pass\n",
    "        error = A[-1] - y\n",
    "        D = [error * self.sigmoid_derivative(A[-1])]\n",
    "        \n",
    "        for layer in np.arange(len(A) - 2, 0, -1):\n",
    "            delta = D[-1].dot(self.W[layer].T)\n",
    "            delta = delta * self.sigmoid_derivative(A[layer])\n",
    "            D.append(delta)\n",
    "            \n",
    "        D = D[::-1]\n",
    "        \n",
    "        #updation of model parameters by gradient descent\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            self.W[layer] += -self.lr * A[layer].T.dot(D[layer])\n",
    "    \n",
    "    def predict(self, X, addBias=True):\n",
    "        p = np.atleast_2d(X)\n",
    "        if addBias:\n",
    "            p = np.c_[p, np.ones((p.shape[0]))]\n",
    "        \n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            p = self.sigmoid(np.dot(p, self.W[layer]))\n",
    "\n",
    "        return p\n",
    "    \n",
    "    def find_loss(self, X, targets):\n",
    "        targets = np.atleast_2d(targets)\n",
    "        predictions = self.predict(X, addBias=False)\n",
    "        loss = 0.5 * np.sum((predictions - targets) ** 2)\n",
    "        return loss\n",
    "    \n",
    "    def fit(self, X, y, epochs=100):\n",
    "        X = np.c_[X, np.ones((X.shape[0]))]\n",
    "        for epoch in np.arange(0, epochs):\n",
    "            \n",
    "            for j in range(0,X.shape[0],self.batch_size):\n",
    "                x = X[j:j+self.batch_size]\n",
    "                target = y[j:j+self.batch_size]\n",
    "                self.train(x, target)\n",
    "                \n",
    "            loss = self.find_loss(X, y)\n",
    "            print(\"Epoch : {}, Loss : {:.7f}\".format(epoch + 1, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6d40025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n",
      "Epoch : 1, Loss : 4564.4701711\n",
      "Epoch : 2, Loss : 3918.8442147\n",
      "Epoch : 3, Loss : 3645.0515401\n",
      "Epoch : 4, Loss : 3405.6107364\n",
      "Epoch : 5, Loss : 3204.1048947\n",
      "Epoch : 6, Loss : 3109.4082497\n",
      "Epoch : 7, Loss : 3020.8689717\n",
      "Epoch : 8, Loss : 2969.3023032\n",
      "Epoch : 9, Loss : 2901.0268678\n",
      "Epoch : 10, Loss : 2852.3293993\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 28 * 28).astype(\"float32\") / 255.0\n",
    "x_test = x_test.reshape(-1, 28 * 28).astype(\"float32\") / 255.0\n",
    "y_train = LabelBinarizer().fit_transform(y_train)\n",
    "y_test = LabelBinarizer().fit_transform(y_test)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "nn = NeuralNet(layers = [x_train.shape[1], 16, 10],batch_size = 16)\n",
    "nn.fit(x_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11df985f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set coding backpropagation from scratch :  0.9359\n"
     ]
    }
   ],
   "source": [
    "predictions = nn.predict(x_test)\n",
    "predictions = predictions.argmax(axis=1)\n",
    "print(\"Accuracy on test set coding backpropagation from scratch : \",accuracy_score(y_test.argmax(axis=1), predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959ee43c",
   "metadata": {},
   "source": [
    "# Training the same neural network using libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fa1ce36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3750/3750 [==============================] - 13s 3ms/step - loss: 3.0696 - accuracy: 0.7390\n",
      "Epoch 2/10\n",
      "3750/3750 [==============================] - 13s 4ms/step - loss: 1.4460 - accuracy: 0.8874\n",
      "Epoch 3/10\n",
      "3750/3750 [==============================] - 13s 4ms/step - loss: 1.1504 - accuracy: 0.9112\n",
      "Epoch 4/10\n",
      "3750/3750 [==============================] - 13s 3ms/step - loss: 0.9806 - accuracy: 0.9248\n",
      "Epoch 5/10\n",
      "3750/3750 [==============================] - 13s 3ms/step - loss: 0.8728 - accuracy: 0.9332\n",
      "Epoch 6/10\n",
      "3750/3750 [==============================] - 13s 3ms/step - loss: 0.7822 - accuracy: 0.9400\n",
      "Epoch 7/10\n",
      "3750/3750 [==============================] - 13s 4ms/step - loss: 0.7108 - accuracy: 0.9463\n",
      "Epoch 8/10\n",
      "3750/3750 [==============================] - 13s 4ms/step - loss: 0.6577 - accuracy: 0.9503\n",
      "Epoch 9/10\n",
      "3750/3750 [==============================] - 14s 4ms/step - loss: 0.6176 - accuracy: 0.9535\n",
      "Epoch 10/10\n",
      "3750/3750 [==============================] - 14s 4ms/step - loss: 0.5640 - accuracy: 0.9575\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ceea1e83a0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    loss = 0.5 * tf.reduce_sum((y_pred - y_true) ** 2)\n",
    "    return loss\n",
    "\n",
    "i = tf.keras.initializers.RandomNormal(mean=0., stddev=1.)\n",
    "model = tf.keras.Sequential([Dense(x_train.shape[1],activation = 'sigmoid',kernel_initializer=i,\n",
    "    bias_initializer=i),\n",
    "                         Dense(16,activation = 'sigmoid',kernel_initializer=i,\n",
    "    bias_initializer=i),\n",
    "                         Dense(10,activation = 'sigmoid',kernel_initializer=i, use_bias=False)])\n",
    "\n",
    "model.compile(loss = custom_loss,optimizer = tf.keras.optimizers.SGD(learning_rate = 0.1),metrics = ['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size = 16,epochs = 10, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81c7e550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set using standard libraries :  0.9398\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_test)\n",
    "predictions = predictions.argmax(axis=1)\n",
    "print(\"Accuracy on test set using standard libraries : \",accuracy_score(y_test.argmax(axis=1), predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bdf6bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
